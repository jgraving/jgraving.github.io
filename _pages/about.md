---
permalink: /
author_profile: true
redirect_from: 
  - /about/
  - /about.html
  - /about
---
<h1 style="font-size: 2.5em; margin-bottom: 0.5em;">Hi, I’m Jake 👋</h1>

<p style="font-size: 1em; line-height: 1.6; max-width: 750px;">
I’m currently a Research Scientist at the <a href="https://www.ab.mpg.de/">Max Planck Institute of Animal Behavior</a>, working at the intersection of AI, data science, and animal behavior.
<br><br>
I build scalable, interpretable tools for collecting and analyzing behavioral data, across both lab and field settings. My work draws from machine learning, computer vision, probabilistic programming, and Bayesian inference to uncover structure in complex systems.
<br><br>
My research is highly interdisciplinary — I collaborate with neuroscientists, biologists, physicists, and engineers to develop end-to-end pipelines spanning experimental design, data collection, and computational analysis.
<br><br>
A central focus of my work is building AI systems that <em>go beyond prediction</em> — tools that help test hypotheses, reveal mechanisms, and support scientific understanding — with the goal of turning computational insights into scientific discoveries.
</p>

<p style="font-size: 1.6em; font-weight: 500; margin: 2em 0 1em 0;">👇 Check out some of my projects below</p>

## 🦗 **Collective Motion in Locusts**  
This study revealed that locust swarming behavior emerges from internal decision-making processes rather than alignment with nearby individuals.
I collaborated with colleagues to analyze behavioral data from controlled lab and immersive virtual reality experiments, developing a Bayesian model to test competing mechanistic hypotheses of collective motion. Our results challenged classical alignment-based theories, showing that group coordination emerges from individual decision-making informed by neural representations of neighboring locusts.

### 🔗 **Links**  
[Publication](https://www.science.org/doi/10.1126/science.adq7832) · [Perspective](https://doi.org/10.1126/science.adw0733) · [Feature Article](https://www.campus.uni-konstanz.de/en/science/scientists-rewrite-the-rules-of-swarming-locusts) · [Model Code](https://github.com/jgraving/sayin_locust_mixture_model)

### 📄 **Citation**  
> Sayin, S., Couzin-Fuchs, E., Petelski, I., Günzel, Y., Salahshour, M., Lee, C.-Y., **Graving, J.M.**, Li, L., Deussen, O., Couzin, I.D., et al. (2025). *The behavioral mechanisms governing collective motion in swarming locusts.* **Science**, 387(6737), 995–1000. [https://doi.org/10.1126/science.adq7832](https://www.science.org/doi/10.1126/science.adq7832)

<div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden; width: 85%; max-width: 100%; margin: 1em auto;">
  <iframe src="https://www.youtube.com/embed/oBJnY4HKmeY" 
          style="position: absolute; top: 0; left: 0; width: 100%; height: 100%;" 
          frameborder="0" allowfullscreen>
  </iframe>
</div>


## 🛸 **Quantifying Group Movement Using Drones and AI**  
This work introduced a drone- and computer vision–based system for high-resolution tracking of animal groups in the wild.
I helped build and refine a scalable pipeline that combines drone video with deep learning and computer vision algorithms to detect, track, and reconstruct 3D movement and environmental context for wild animals. Working with an interdisciplinary team, we enabled large-scale, non-invasive behavioral analysis in natural settings — advancing tools for behavioral ecology and conservation.

### 🔗 **Links**  
[Code on GitHub](https://github.com/benkoger/overhead-video-worked-examples) · [Publication](https://besjournals.onlinelibrary.wiley.com/doi/10.1111/1365-2656.13904) · [Press Release](https://www.biologie.uni-konstanz.de/fachbereich/aktuelles/details/observing-group-living-animals-with-drones-and-computer-vision/) · [Feature Article](https://www.campus.uni-konstanz.de/en/science/observing-group-living-animals-with-drones-and-computer-vision) · [HerdHover Project](https://herdhover.com/)

### 📄 **Citation**  
> Koger, B., Deshpande, A., Kerby, J.T., **Graving, J.M.**, Costelloe, B.R., & Couzin, I.D. (2023). *Quantifying the movement, behaviour and environmental context of group-living animals using drones and computer vision.* **Journal of Animal Ecology**, 92(7), 1498–1515. [https://doi.org/10.1111/1365-2656.13904](https://besjournals.onlinelibrary.wiley.com/doi/10.1111/1365-2656.13904)

<img src="https://raw.githubusercontent.com/jgraving/jgraving.github.io/master/files/images/tracks_on_map_observation088.png" alt="Animal Tracks on Map" style="width:100%; margin-top:10px;">


## 🐟 **Vortex Phase Matching in Schooling Fish**  
This paper identified a schooling strategy where fish synchronize their tailbeats to exploit hydrodynamic advantages, matching behavior previously seen in bio-inspired robots.
I contributed to the analysis pipeline, applying deep learning–based pose estimation to track fish movement and test predictions from robotic models. The findings showed that real fish use similar vortex-phase matching strategies to gain energetic benefits during coordinated swimming.

### 🔗 **Links**  
[Publication](https://www.nature.com/articles/s41467-020-19086-0) · [Press Release](https://www.uni-konstanz.de/en/university/news-and-media/current-announcements/news-in-detail/roboter-helfen/)

### 📄 **Citation**  
> Li, L., Nagy, M., **Graving, J.M.**, Bak-Coleman, J., Xie, G., & Couzin, I.D. (2020). *Vortex phase matching as a strategy for schooling in robots and in fish.* **Nature Communications**, 11, Article 5408. [https://doi.org/10.1038/s41467-020-19086-0](https://www.nature.com/articles/s41467-020-19086-0)

<div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden; width: 85%; max-width: 100%; margin: 1em auto;">
  <iframe src="https://www.youtube.com/embed/J4T4hi8RO7s" 
          style="position: absolute; top: 0; left: 0; width: 100%; height: 100%;" 
          frameborder="0" allowfullscreen>
  </iframe>
</div>


## 🤸‍♂️ **DeepPoseKit**  
DeepPoseKit is an open-source software toolkit for fast, accurate animal pose estimation using a deep learning–based, multi-scale model.
I led the development of DeepPoseKit to make deep learning–based posture tracking more accessible and efficient for behavioral researchers. The toolkit supports fast, accurate pose estimation across species — even with small training datasets — and has been adopted by labs working in both lab and field contexts.

### 🔗 **Links**  
[Code on GitHub](https://github.com/jgraving/DeepPoseKit) · [Publication](https://elifesciences.org/articles/47994) · [Feature Article](https://www.campus.uni-konstanz.de/en/science/new-method-improves-measurement-of-animal-behaviour-using-deep-learning)

### 📰 **Featured In**  
[Quanta Magazine](https://www.quantamagazine.org/to-decode-the-brain-scientists-automate-the-study-of-behavior-20191210/) · [Nature Methods](https://doi.org/10.1038/s41592-019-0678-2) · [Nature News & Views](https://doi.org/10.1038/d41586-019-02942-5)

### 📄 **Citation**  
> **Graving, J.M.**, Chae, D., Naik, H., Li, L., Koger, B., Costelloe, B.R., Couzin, I.D. (2019). *DeepPoseKit, a software toolkit for fast and robust animal pose estimation using deep learning.* **eLife**, 8, e47994. [https://doi.org/10.7554/eLife.47994](https://elifesciences.org/articles/47994)

<img src="https://raw.githubusercontent.com/jgraving/jgraving.github.io/master/files/images/Figure1video1.gif" alt="Swarm Pose" style="width:100%; margin-top:10px;">


## 🐦 **Automated Barcode Tracking in Birds**  
This system uses barcode-equipped backpacks to enable automated, individual-level tracking of birds for behavioral research.
As part of a collaborative project, I helped design and implement the computer vision pipeline used to detect and track barcoded zebra finches. The system provides continuous identity, position, and orientation data, supporting long-term studies of behavior and social networks at high resolution.

### 🔗 **Links**  
[Publication](https://doi.org/10.1111/2041-210X.13005)

### 📄 **Citation**  
> Alarcón-Nieto, G., **Graving, J.M.**, Klarevas-Irby, J.A., Maldonado-Chaparro, A.A., Mueller, I., & Farine, D.R. (2018). *An automated barcode tracking system for behavioural studies in birds.* **Methods in Ecology and Evolution**, 9(6), 1536–1547. [https://doi.org/10.1111/2041-210X.13005](https://doi.org/10.1111/2041-210X.13005)

<div style="text-align: center;">
  <video width="85%" controls>
    <source src="https://github.com/jgraving/jgraving.github.io/raw/master/files/videos/zebra_finches_tracked.mp4" type="video/mp4">
    Your browser does not support the video tag.
  </video>
</div>


## 🕷 **Navigation in Whip Spiders**  
Our studies revealed that whip spiders use tactile and olfactory cues — not vision — to navigate complex environments with impressive spatial precision.
I led experimental design and data collection across both lab and field settings, collaborating with colleagues to combine radio telemetry, behavioral tracking, and sensory manipulations. Together, we showed that antenniform legs provide essential multisensory input for goal-directed navigation in the absence of visual cues.

### 🔗 **Links**  
[Journal of Experimental Biology (2017)](https://doi.org/10.1242/jeb.149823) · [Journal of Comparative Physiology A (2017)](https://doi.org/10.1007/s00359-017-1169-5) · [Frontiers in Behavioral Neuroscience (2016)](https://doi.org/10.3389/fnbeh.2016.00047)

### 📄 **Citations**  
> **Graving, J.M.**, Bingman, V.P., Hebets, E.A., & Wiegmann, D.D. (2017). *Development of site fidelity in the nocturnal amblypygid, Phrynus marginemaculatus.* **Journal of Comparative Physiology A**, 203, 313–328. [https://doi.org/10.1007/s00359-017-1169-5](https://doi.org/10.1007/s00359-017-1169-5)  
>  
> Bingman, V.P., **Graving, J.M.**, Hebets, E.A., & Wiegmann, D.D. (2017). *Importance of the antenniform legs, but not vision, for homing by the neotropical whip spider Paraphrynus laevifrons.* **Journal of Experimental Biology**, 220, 885–890. [https://doi.org/10.1242/jeb.149823](https://doi.org/10.1242/jeb.149823)  
>  
> Wiegmann, D.D., Hebets, E.A., Gronenberg, W., **Graving, J.M.**, & Bingman, V.P. (2016). *Amblypygids: Model organisms for the study of arthropod navigation mechanisms in complex environments?* **Frontiers in Behavioral Neuroscience**, 10:47. [https://doi.org/10.3389/fnbeh.2016.00047](https://doi.org/10.3389/fnbeh.2016.00047)

<div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden; width: 85%; max-width: 100%; margin: 1em auto;">
  <iframe src="https://www.youtube.com/embed/6hY8X-qwyiU" 
          style="position: absolute; top: 0; left: 0; width: 100%; height: 100%;" 
          frameborder="0" allowfullscreen>
  </iframe>
</div>

<!-- 
Optional additional gifs:
<img src="https://raw.githubusercontent.com/jgraving/jgraving.github.io/master/files/images/zebra.gif" alt="Zebra Pose Estimation" style="width:25%;">
<img src="https://raw.githubusercontent.com/jgraving/jgraving.github.io/master/files/images/locust.gif" alt="Locust Pose Estimation" style="width:25%;">
-->
